\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amsfonts}
\usepackage{bbm}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{moreverb}
%\usepackage[ruled, vlined]{algorithm2e}
\usepackage{cite}
\usepackage{mathtools}
\usepackage[font=small]{caption}
\usepackage{lineno}
\linenumbers

\usepackage{natbib}


\bibliographystyle{unsrt}
\renewcommand{\bibsection}{} % Do not print the reference section name so we can make it ourselves.

\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\renewcommand\linenumberfont{\normalsize}
\newcommand*\patchAmsMathEnvironmentForLineno[1]{%
  \expandafter\let\csname old#1\expandafter\endcsname\csname #1\endcsname
  \expandafter\let\csname oldend#1\expandafter\endcsname\csname end#1\endcsname
  \renewenvironment{#1}%
     {\linenomath\csname old#1\endcsname}%
     {\csname oldend#1\endcsname\endlinenomath}}% 
\newcommand*\patchBothAmsMathEnvironmentsForLineno[1]{%
  \patchAmsMathEnvironmentForLineno{#1}%
  \patchAmsMathEnvironmentForLineno{#1*}}%
\AtBeginDocument{%
\patchBothAmsMathEnvironmentsForLineno{equation}%
\patchBothAmsMathEnvironmentsForLineno{align}%
\patchBothAmsMathEnvironmentsForLineno{flalign}%
\patchBothAmsMathEnvironmentsForLineno{alignat}%
\patchBothAmsMathEnvironmentsForLineno{gather}%
\patchBothAmsMathEnvironmentsForLineno{multline}%
}


% Font

%\renewcommand{\familydefault}{\sfdefault}



\author{Surojit Biswas, Konstantin Kerner, Paulo Jos\'{e} Pereira Lima Texeira, \\ Jeffery L. Dangl, Vladimir Jojic, Philip A. Wigge}
\date{}


\title{Tradict - mathematical details}

\begin{document}
\maketitle

\tableofcontents
\vspace{10mm}

This document describes the full mathematical details for the concepts presented in the ``Tradict algorithm'' section, ``Building a predictive Multivariate Normal Continuous-Poisson hierarchical model''  subsection of the Materials and Methods in the Supplemental Information. Specifically, we present exactly how Tradict uses a selected set of markers to 1) complete training, and 2) to perform prediction. 

\section{Preliminaries} \label{prelim}

For a matrix $A$, $A_{:i}$ and $A_{i:}$ index the $i^{th}$ column and row, respectively. For a set of indices, $q$, we use $-q$ to refer to all indices not specified by $q$. 

\section{Model} \label{model}

Tradict uses a Continuous-Poisson Multivariate Normal (CP-MVN) hierarchical model to model the expression of transcriptional programs and all genes in the transcriptome. Multivariate Normal hierarchies have been explored in the past as a means of modeling correlation structure among count based random variables [REF]. However, given we will be working with abundances as transcripts per million (TPM), which are non-negative (can equal zero) and fractional, we  relax the integral assumption of the Poisson so it is continuous on $[0, \infty)$.   Specifically, we define the continuous relaxation of the Poisson distribution (hereafter, Continuous-Poisson) to have the following density function:
\[
f(x|\lambda) = C_\lambda \frac{e^{-\lambda} \lambda^x}{\Gamma(x + 1)}
\]
where $C_\lambda$ is a normalization constant [REF]. The mean of this distribution is given by $\lambda$, just as the Poisson.

We begin by building a predictive model of gene expression, and thereafter discuss a predictive model for the expression of transcriptional programs. Let $z_j$ denote the log-\emph{latent abundance} of gene $j$, such that $\exp(z_j)$ is the \emph{latent abundance} of that gene (in TPM) whose measured abundance is given by $t_j$. Let $T_j = t_jo$ be the measured total number of transcripts of gene $j$. Here o is the sequencing depth in millions of reads of the sample under consideration. We assume then, 
\begin{align*}
z & \sim \mathcal{N}\left(\mu, \Sigma\right) \\
T_j & \sim \textrm{Continuous-Poisson}(\exp(z_j)o)
\end{align*}
where $\mu$ and $\Sigma$ are of dimension 1 $\times$ $\#$-genes and $\#$-genes $\times$ $\#$-genes, respectively. In effect, we are assuming that the measured number of transcripts for gene $j$ is a noisy realization of a latent abundance $\exp(z_j)$ times the sequencing depth, $o$. The dependencies between log-latent abundances (the $z_j$'s) are then encoded by the covariance matrix of the Multivariate Normal layer of the model. 


Note that we could model the TPM measurements directly in the second layer by assuming $t_j \sim \textrm{Continuous-Poisson}(\exp(z_j))$; however, this formulation does not consider sequencing depth, which can be a valuable source of information when inferring latent abundances for rare/poorly sampled genes \cite{Biswas2016a}. 

During decoding, we are interested in building a predictive model between markers and all genes in the transcriptome. Therefore, we need to consider a conditional model of the transcriptome given the log-latent abundances of the markers. Let $m$ be the set of indices for the given panel of selected markers, which are the subset of genes Tradict selects as representative of the transcriptome. To perform prediction we therefore need $p(z_{-m} | z_m)$, and given this we would like to ultimately compute as our estimate of the abundance of all genes in the transcriptome $\hat{T} = \argmax_T p(T | z_m)$.  We have, 
\begin{align*}
z_m & \sim \mathcal{N}(\mu^{(m)}, \Sigma^{(m)}) \\
z_{-m}|z_m & \sim \mathcal{N}(\mu_{z_{-m}|z_m}, \Sigma_{z_{-m}|z_m}) \\
T_j & \sim \textrm{Continuous-Poisson}(\exp(z_j)o)
\end{align*}

Here, $\mu^{(m)}$ and $\Sigma^{(m)}$ refer to mean vector and covariance matrix of $z_m$. Given these, the conditional mean of the log-latent abundances for all non-marker genes can be obtained through Gaussian conditioning. Specifically, for two normally distributed row-vector variables $a$ and $b$ the conditional mean of $b$ given $a$ is given by $\mu_{b|a} = \mu_b + (a - \mu_a)\Sigma_{a}^{-1}\sigma_{ab}$ and $\Sigma_{b|a} = \Sigma_{b} - \sigma_{ab}^T\Sigma_a^{-1}\sigma_{ab}$, where $\sigma_{ab}$ is the cross-covariance between $a$ and $b$, and $\Sigma_a$ and $\Sigma_b$ are the covariance matrices of $a$ and $b$, respectively.

Given the expression of a transcriptional program is a linear combination of the latent abundances of its constituent genes, they will be normally distributed given 1) Central Limit Theorem, and 2) the latent abundances themselves are normally distributed (convolutions of normals are normals). Let $s$ be the expression of all transcriptional programs. We posit the following model, 
\begin{align*}
z_m & \sim \mathcal{N}\left(\mu^{(m)}, \Sigma^{(m)} \right) \\
s|z_m & \sim \mathcal{N}(\mu_{s|z_m}, \Sigma_{s|z_m}) 
\end{align*}
To use these models for prediction, we must learn their parameters from training data. This would complete the process of encoding described in the Supplemental Information. Specifically, we need to learn $\mu^{(m)}$, $\Sigma^{(m)}$, $\mu_s$, $\mu_{z_{-m}}$, $\sigma_{z_m,s}$ and $\sigma_{z_m,z_{-m}}$.

\section{Training} \label{inference}

As described in the Supplemental Information, given an estimate of $z_m$, $\hat{z}_{m}$, inference of $\mu_s$, $\mu_{z_{-m}}$, $\sigma_{z_m,s}$ and $\sigma_{z_m,z_{-m}}$ is straightforward. In lag transforming the entire training TPM expression matrix, $t \in \mathbb{R}^{\textrm{samples} \times \textrm{genes}}$,  we have an estimate of $z$, $\hat{z} = \textrm{lag}(t)$ \cite{Biswas2016a}. Thus, an estimate of $\mu_{z_{-m}}$ is given by the usual column-wise sample mean of $\hat{z}_{-m}$. 

Let $\Lambda \in \mathbb{R}^{\textrm{genes} \times \textrm{transcriptional programs}}$ be a matrix of principal component 1 coefficients over genes for each transcriptional program. Note, that $\Lambda_{ij} = 0$ if gene $i$ is not in transcriptional program $j$. An estimate of $s$ is given by $\hat{s} = \hat{z}\Lambda$, and so an estimate for $\mu_s$, $\hat{\mu}_s$, is given by the usual column-wise mean of $\hat{s}$. 

Given $\hat{z}_m$ the cross-covariances, $\sigma_{z_m,s}$ and $\sigma_{z_m,z_{-m}}$, are given by the usual sample cross-covariance between $\hat{z}_m$ and $\hat{s}$ and between $\hat{z}_m$ and $\hat{z}_{-m}$, respectively. 

Now, though we could use the lag-transformed values of $t_m$ as our estimate for $z_m$, we have an opportunity to improve this estimate by virtue of having to estimate $\mu^{(m)}$ and  $\Sigma^{(m)}$. More specifically, given $z_m$, estimates of $\mu^{(m)}$ and  $\Sigma^{(m)}$ are given by -- up to some regularization -- the usual sample mean and covariance of $z_m$. Furthermore, given $\mu^{(m)}$ and  $\Sigma^{(m)}$, we can update our estimate of $z_m$ to the maximum of its posterior distribution. This suggests an alternating iterative procedure in which we iterate 1) estimation of $\mu^{(m)}$ and  $\Sigma^{(m)}$, and 2) maximum \emph{a posteriori} inference of $z_m$ until convergence of their joint likelihood. It is the $\hat{z}_m$ that we obtain from this procedure that we use in the cross-covariance calculations above. The following section details this procedure.



\subsection{Inference of $z_m$ given $\mu^{(m)}$ and $\Sigma^{(m)}$}

Suppose Tradict has estimates of $\mu^{(m)}$ and $\Sigma^{(m)}$ given by $\hat{\mu}^{(m)}$ and $\hat{\Sigma}^{(m)}$, and let $T_m = t_m(o \times \textbf{1}_{1 \times \textrm{markers}} )$ be a matrix of the total measured number of transcripts for each marker. Here $o \in \mathbb{R}^{\textrm{samples} \times 1}$ is a vector of sample sequencing depths in millions of reads. Given these, we would like to calculate the maximum \emph{a posteriori} (MAP) estimate of $\hat{z}_m = \argmax_{z_m} p(z_m | o, T_m, \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)}) $. 

The posterior distribution over $z_m$ is given by 
\begin{align*}
p(z_m| o, T_m, \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)}) & =  \frac{p(T_m | o, z_m, \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)}) p(z_m| \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)}) }{ \int_k  p(T_m | o, k, \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)}) p(k | \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)}) \textrm{d}k } \\
& \propto \prod_{i=1}^{n}  p(T_{im} | o, z_{im}, \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)}) p(z_{im}| \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)}) \\
& = \prod_{i=1}^{n} \left[ \prod_{j=1}^{|m|} C_{[\exp(z_{ij})o_i]} [\exp(z_{ij})o_i]^{T_{ij}}  e^{-[\exp(z_{ij})o_i]} / \Gamma(T_{ij} + 1) \right] \\ 
& \times \frac{1}{\sqrt{2\pi|\hat{\Sigma}^{(m)}|}^{|m|}}\exp\left(-\frac{1}{2}(z_{i:}-\hat{\mu}^{(m)}) \textrm{inv}\left( \hat{\Sigma}^{(m)} \right)(z_{i:}-\hat{\mu}^{(m)})^T \right)
\end{align*}
where for notational clarity we have used $\textrm{inv}(\cdot)$ to represent matrix inverse. 

Given $z$ is a matrix parameter, this may be difficult to solve directly. However, note that given $z_{ij}$, $T_{ij}$ is conditionally independent of $T_{i,-j}$. Additionally, given $z_{i,-j}$, $z_{ij}$ is normally distributed with mean and covariance 
\begin{align*}
a_{ij} & = \mu^{(m)}_j + \left(z_{i,-j} - \mu^{(m)}_{-j} \right)\textrm{inv}\left( \Sigma^{(m)}_{-j,-j} \right) \Sigma^{(m)}_{-j,j} \\
\sigma_{m(j)} &= \Sigma^{(m)}_{j,j} - \Sigma^{(m)}_{j,-j}\textrm{inv}\left( \Sigma^{(m)}_{-j,-j} \right) \Sigma^{(m)}_{-j,j}
\end{align*}
respectively. Taken together, this suggests an iterative conditional modes algorithm \cite{Besag1986} in which we maximize the posterior one column of $z$ at a time, while conditioning on all others. 

Let $\hat{z}_m$ denote our current estimate of $z_m$. Let $m(j)$ denote the index of the $j^{th}$ marker and let $m(-j)$ denote the indices of all markers but the $j^{th}$ one. The above sub-objective is given by,
\begin{align*}
\hat{z}_{im(j)} & = \argmax_{z_{im(j)} | z_{im(-j)}} \log p(z_{im(j)} | T_{im(j)}, o_i, \hat{z}_{im(-j)}, \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)}) \\
& = \argmax_{z_{im(j)} | z_{im(-j)} } \log p(T_{im(j)} | z_{im(j)},  o_i, \hat{z}_{im(-j)}, \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)})p(z_{im(j)} | \hat{z}_{im(-j)}, \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)})  \\
& = \argmax_{z_{im(j)} | z_{im(-j)} } \log p(T_{im(j)} | z_{im(j)}, o_i)p(z_{im(j)} | \hat{z}_{im(-j)}, \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)})  \\
& = \argmax_{z_{im(j)} | z_{im(-j)} } \log \left[ [\exp(z_{im(j)})o_i]^{T_{im(j)}}  e^{-[\exp(z_{im(j)})o_i]} \exp \left( -\frac{1}{2\sigma_{m(j)}}(z_{im(j)} - a_{im(j)})^2 \right) \right] \\
& = \argmax_{z_{im(j)} | z_{im(-j)} }  T_{im(j)}\exp(z_{im(j)})o_i -\exp(z_{im(j)})o_i  -\frac{1}{2\sigma_{m(j)}}(z_{im(j)} - a_{im(j)})^2 
\end{align*}


Differentiating we get,
\begin{align*}
\frac{\partial}{\partial z_{im(j)}}  T_{im(j)}z_{im(j)}o_i - \exp(z_{im(j)})o_i  -\frac{1}{2\sigma_{m(j)}}(z_{im(j)} - a_{im(j)})^2 \\ 
= T_{im(j)}o_i - \exp(z_{im(j)})o_i  -\frac{1}{\sigma_{m(j)}}(z_{im(j)} - a_{im(j)}) \\ 
\end{align*}
Because $z_{im(j)}$ appears as a linear and exponential term, we cannot solve this gradient analytically. We therefore utilize Newton-Raphson optimization. For this we also require the Hessian, which is given by,  
\begin{align*}
\frac{\partial}{\partial z_{im(j)}} T_{im(j)}o_i - \exp(z_{im(j)})o_i  -\frac{1}{\sigma_{m(j)}}(z_{im(j)} - a_{im(j)}) \\ 
=  -\exp(z_{im(j)})o_i  - \frac{1}{\sigma_{m(j)}} < 0
\end{align*}
Notice the Hessian is always negative-definite, which implies each update has a single, unique optimum. 

In practice, the Newton-Raphson updates can be performed in vectorized fashion iteratively for each column of $z$. We generally find that this optimization takes 5-15 iterations (full passes over all columns of $z$) and less than a minute to converge.  We refer to the program that performs these calculations as $\hat{z}_m = \verb|MAP_z| \left(t, o, \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)} \right)$.


\subsection{Complete inference of $\mu^{(m)}$, $\Sigma^{(m)}$, and $z_m$}

For complete inference we use the following iterative conditional modes algorithm \cite{Besag1986}:
\begin{itemize}
\item Initialize $T_m = t_m(o \times \textbf{1}_{1 \times \textrm{markers}} )$, $\hat{z}_m = \textrm{lag}(t_m)$.
\item Until convergence of $\log p(T_m | o, \hat{z}_m, \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)}) + \log p(\hat{z}_m| \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)})$, iterate:
\begin{itemize}
\item Update $\hat{\mu}^{(m)}$ and $\hat{\Sigma}^{(m)}$:
\begin{align*}
\hat{\mu}^{(m)} & = \frac{1}{\#\textrm{samples}} \sum_i \hat{z}_{im} \\
\hat{\Sigma}^{(m)} & = \frac{1}{\#\textrm{samples} - 1} \sum_i (\hat{z}_{im} - \hat{\mu}^{(m)})^T(\hat{z}_{im} - \hat{\mu}^{(m)}) + \lambda\textrm{diag}\left[\textrm{cov}\left(\hat{z}_m^{(\textrm{init})} \right) \right]
\end{align*}
\item Update  $\hat{z}_m = \verb|MAP_z| \left(t, o, \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)} \right)$.
\end{itemize}
\end{itemize}
Here $\textrm{diag}(x)$ of the square matrix $x$ returns an equivalently sized matrix with only the diagonal of $x$ preserved and 0's for the off-diagonal terms. $\textrm{cov}(\cdot)$ denotes the usual sample covariance matrix.

Note that in this algorithm we have added a regularization to the estimate of the covariance matrix. This is done in order to ensure stability and to avoid infinite-data-likelihood singularities that arise from singular covariance matrices. This is most often happens when a gene's TPM abundance is mostly zero (i.e. there is little data for the gene), giving the multivariate normal layer an opportunity to increase the data likelihood (via the determinant of the covariance matrix) by tightly coupling this gene's latent abundance to that of another gene, thereby producing a singularity. This regularization is probabilistically equivalent to adding an Inverse-Wishart prior over $\Sigma^{(m)}$. The parameter $\lambda$ controls the strength of the regularization. In practice, we find $\lambda = 0.1$ leads to good predictive performance, stable (non-singular) covariance matrices, and reasonably quick convergence.





\section{Prediction}

During prediction we are given new measured TPM measurements for our markers, $t^*_m \in \mathbb{R}^{\textrm{query samples} \times |m|} $, and we must make predictions about the expression of all transcriptional programs and the remaining non-marker genes. We have two options available to us: 1) Calculate a point (MAP) estimate or 2) calculate the complete posterior distribution over each non-marker gene and transcriptional program in a fully Bayesian manner. The former option is faster, but the second gives more information on the uncertainty of the prediction. We therefore implement both options in Tradict and detail their derivation below. Note that knowing the entire posterior distribution allows one to derive whatever estimator they would like, and so option 2, informationally speaking, supersets option 1. 

\subsection{MAP estimation of gene and program abundances}
We first need an estimate of the log-latent abundances $\hat{z}_m^*$ associated with $t^*_m $. Given the estimates $\hat{\mu}^{(m)}$ and $\hat{\Sigma}^{(m)} $ obtained from the training data, we obtain these estimates as 
\[
\hat{z}_m^* = \verb|MAP_z| \left(t_m^*, \textbf{1}_{\textrm{query samples} \times 1}, \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)} \right)
\]


 %Because the sequencing depths for these measurements won't be directly comparable to those required for full transcriptome sequencing, we do not consider these. Given a fraction of the reads required for full transcriptome sequencing will be needed to only measure marker gene expression, we assume that the measurements are thorough. In this case the effect of sequencing depth on the inference of log-latent abundances will be small, if we directly model $\mathbb{E}[t^*_j | z_j] = \exp((z_j)\times 1)$ 

Given the inferred marker latent abundances, we let our estimates of $s^*$ and $t_m^*$ be the maximizers of their probability distribution. In other words, $\hat{s}^* = \argmax_{s^*} p(s^* | \hat{z}_m^*)$ and $\hat{t}_m^* = \argmax_{t_m^*} p(t_m^* | \hat{z}_m^*)$. 

Our estimate for the expression of all transcriptional programs is given by
\[
\argmax_{s^*} p(s^* | \hat{z}_m^*) = \mathbb{E}[s^* |\hat{z}_m^* ] = \mu_{s^* |\hat{z}_m^* } = \hat{\mu}_s + \left(\hat{z}_m^*  - \hat{\mu}^{(m)} \right)\textrm{inv}\left( \hat{\Sigma}^{(m)} \right)\hat{\sigma}_{z_{m},s}.
\]
Here, $\hat{\mu}_s $ and $\hat{\sigma}_{z_{m},s}$ represent estimates of the unconditional mean of $s$ and the cross-covariance matrix between $z_m$ and $s$ previously learned during encoding.

Similarly, for the entire transcriptome we have, 
\[
\hat{t}^*_{ij} = \argmax_{t} p(t |  \hat{z}^*_{im})  = \exp\left( \mu_{z_{ij} | \hat{z}^*_{im}}  \right).
\]
where, 
\[
\mu_{z_{ij} | \hat{z}^*_{im}} =  \hat{\mu}_j + \left( \hat{z}^*_{im}- \hat{\mu}^{(m)} \right)\textrm{inv}\left( \hat{\Sigma}^{(m)} \right)\hat{\sigma}_{z_{m},z_j} 
\]


We could also use the expected value of $t$ as our estimate. 
\begin{align*}
\mathbb{E}[t^*_{ij}  | \hat{z}^*_{im}] & =  \int_{-\infty}^\infty \mathbb{E}[t^*_{ij} | z^*_{ij}] p(z_{ij} | \hat{z}^*_{im})  \textrm{d}z_{ij} \\
& = \int_{-\infty}^\infty \exp(z_{ij}) \mathcal{N}(z_{ij} | \mu_{z_{ij} | \hat{z}_{im}^* }, \Sigma_{z_{ij} | \hat{z}_{im}^* }) \textrm{d}z_{ij} \\
& = \mathbb{E}_{\mathcal{N}}[\exp(z_{ij}) | \hat{z}^*_{im}]
\end{align*}
The Moment Generating Function of a Normal random variable $X$ with mean $\mu$ and variance $\sigma^2$ is given by $M(t) = \mathbb{E}[\exp(tX)] = \exp(\mu t + \sigma^2t^2/2)$. Therefore we have, 
\[
\mathbb{E}[t^*_{ij}  | \hat{z}^*_{im}] = \mathbb{E}_{\mathcal{N}}[\exp(z_{ij}) | \hat{z}^*_{im}] = M(1) = \exp \left( \mu_{z_{ij} | \hat{z}_{im}^* } + \frac{1}{2}\Sigma_{z_{ij} | \hat{z}_{im}^* } \right)
\]
where, 
\begin{align*}
\mu_{z_{ij} | \hat{z}^*_{im}} &=  \hat{\mu}_j + \left( \hat{z}^*_{im}- \hat{\mu}^{(m)} \right)\textrm{inv}\left( \hat{\Sigma}^{(m)} \right)\hat{\sigma}_{z_{m},z_j} \\
\Sigma_{z_{ij} | \hat{z}^*_{im}} &=  \hat{\sigma}_{jj} -  \hat{\sigma}_{z_{m},z_j}^T\textrm{inv}\left( \hat{\Sigma}^{(m)} \right)\hat{\sigma}_{z_{m},z_j}
\end{align*}
Here, $\hat{\mu}_j $ and $\hat{\sigma}_{z_{m},z_j}$ represent estimates of the unconditional mean of $z_j$ and the cross-covariance matrix between $z_m$ and $z_j$. These were learned from the training data during encoding.

Though this predictor is unbiased, it does not produce a good prediction for most samples. This is due to the right-skew of the Poisson, which drags its mean away from the most likely values. 

\subsection{Posterior density estimation of gene and program abundances}

The above predictions represent point estimates. Ideally, we would like to know the uncertainty around these estimates. Given measurements of the representative markers, we can estimate the posterior distribution of expression values for transcriptional programs and the non-markers, and therein calculate any point estimates and/or measures of uncertainty. Recall that for transcriptional programs: 
\begin{align*}
z_m & \sim \mathcal{N}\left(\mu^{(m)}, \Sigma^{(m)} \right) \\
s|z_m & \sim \mathcal{N}(\mu_{s|z_m}, \Sigma_{s|z_m}) 
\end{align*}
And similarly for genes (among which the marker genes are included) we have: 
\begin{align*}
z_m & \sim \mathcal{N}(\mu^{(m)}, \Sigma^{(m)}) \\
z_{-m}|z_m & \sim \mathcal{N}(\mu_{z_{-m}|z_m}, \Sigma_{z_{-m}|z_m}) \\
T_j & \sim \textrm{Continuous-Poisson}(\exp(z_j)o)
\end{align*}

Given $z_m$, the distribution of expression values are simple normal distributions with analytically available means and covariances. However, because $z_m$ is unknown, we must factor into our estimate its distribution, which is both a function of observed data ($t_m$, $o$) and prior information (in the form of $\hat{\mu}^{(m)}$ and $\hat{\Sigma}^{(m)}$. Our strategy  to estimate the posterior density of programs and non-markers will therefore be to sample from the posterior of $z_m$, and then given these draws, sample from the conditional Normal distribution of each program and non-marker gene.

\subsubsection{Sampling $z_m$ via MCMC} 

To sample $z_m$ we use Metropolis-Hastings Markov Chain Monte Carlo (MCMC) sampling [REF], using the following posterior density function:

\begin{align*}
p(z_m| o, T_m, \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)}) & =  \frac{p(T_m | o, z_m, \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)}) p(z_m| \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)}) }{ \int_k  p(T_m | o, k, \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)}) p(k | \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)}) \textrm{d}k } \\
& \propto \prod_{i=1}^{n}  p(T_{im} | o, z_{im}, \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)}) p(z_{im}| \hat{\mu}^{(m)}, \hat{\Sigma}^{(m)}) \\
& = \prod_{i=1}^{n} \left[ \prod_{j=1}^{|m|} C_{[\exp(z_{ij})o_i]} [\exp(z_{ij})o_i]^{T_{ij}}  e^{-[\exp(z_{ij})o_i]} / \Gamma(T_{ij} + 1) \right] \\ 
\end{align*}
Note that we do not require the marginal distribution for Metropolis-Hastings sampling. 

As our proposal distribution we use:
\[
z_m^{(i+1)} = \mathcal{N}\left( z_m^{(i)}, \gamma\mathbb{I}_{|m|\times |m|}\right).
\]
Here $z^{(i)}$ is the $i^{th}$ draw from the sampler, and $\gamma$ represents the width (variance) of the proposal distribution. To choose this width, we examine, for a schedule of proposal widths (50 logarithmically spaced widths between $10^{3.5}$ and $10^{-1}$), which width gives an acceptance rate closest to 0.4 [REF]. Using this width, we sample 20,000 times from the sampler. We burn-in the first 100 samples and keep every 100$^{th}$ sample thereafter (to offset the effects of the chain's auto-correlation) as our draws from the distribution. Note that we initialize the chain at the MAP estimate of $z_m$. This ensures the chain is stationary from the beginning.

\subsubsection{Sampling program and gene abundances} 
Given our $M = 200$ draws, $\left[z^{(i)}_m\right]_{i=1}^{M}$, we can sample from the conditional distribution of each program and gene.

Our $i^{(th)}$ draw from the posterior distribution over all programs is obtained from sampling the following Multivariate-Normal,
\begin{align*}
s^{(i)}|z^{(i)}_m & \sim \mathcal{N}\left(\mu_{s| z_{m}^{(i)}}, \Sigma_{s| z_{m}^{(i)}}  \right) 
\end{align*}
where
\begin{align*}
\mu_{s| z_{m}^{(i)}}  &= \hat{\mu}_s + \left(z_m^{(i)}  - \hat{\mu}^{(m)} \right)\textrm{inv}\left( \hat{\Sigma}^{(m)} \right)\hat{\sigma}_{z_{m},s} \\
\Sigma_{s| z_{m}^{(i)}} & = \hat{\Sigma}_{s} -  \hat{\sigma}_{z_{m},s}^T  \textrm{inv}\left( \hat{\Sigma}^{(m)} \right) \hat{\sigma}_{z_{m},s}
\end{align*}

Similarly, our $i^{(th)}$ draw from the posterior distribution over all genes \emph{could be} obtained from sampling the following Multivariate-Normal,
\begin{align*}
z_{-m}^{(i)}|z^{(i)}_m & \sim \mathcal{N}\left(\mu_{z_{-m}| z_{m}^{(i)}}, \Sigma_{z_{-m}| z_{m}^{(i)}}  \right) 
\end{align*}
where
\begin{align*}
\mu_{z_{-m}| z_{m}^{(i)}}  &= \hat{\mu}_{z_{-m}} + \left(z_m^{(i)}  - \hat{\mu}^{(m)} \right)\textrm{inv}\left( \hat{\Sigma}^{(m)} \right)\hat{\sigma}_{z_{m},z_{-m}} \\
\Sigma_{z_{-m}| z_{m}^{(i)}} & = \hat{\Sigma}_{z_{-m}} -  \hat{\sigma}_{z_{m},z_{-m}}^T  \textrm{inv}\left( \hat{\Sigma}^{(m)} \right) \hat{\sigma}_{z_{m},z_{-m}}
\end{align*}
However, given the size of $\Sigma_{z_{-m}| z_{m}^{(i)}}$ (approximately $21000 \times 21000$), this is not easily doable. Recall, though, that one of our basic assumptions is that the conditional mean abundance of all genes given the abundance of our markers has the covariance structure of all genes sufficiently built in. Thus, we assume 
\[
\mathcal{N}\left(\mu_{z_{-m}| z_{m}^{(i)}}, \Sigma_{z_{-m}| z_{m}^{(i)}}  \right) \approx \mathcal{N}\left(\mu_{z_{-m}| z_{m}^{(i)}}, \textrm{diag}\left(\Sigma_{z_{-m}| z_{m}^{(i)}} \right) \right)
\]
Here $\textrm{diag}(\cdot)$ replaces all off-diagonal entries with zeros. Consequently, we only need to compute the diagonal entries of the conditional covariance matrix, and futhermore, given the conditional mean of each, we can sample it's abundance in parallel and independently of all others.






\section{References}
\bibliography{refs}

\end{document}